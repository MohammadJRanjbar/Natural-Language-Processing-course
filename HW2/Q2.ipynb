{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK9_X6CBfqMP",
        "outputId": "7925194f-dfe2-4ed0-b613-be12889f1a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Finance.zip\n",
            "  inflating: FinancialPhraseBank/License.txt  \n",
            "  inflating: FinancialPhraseBank/README.txt  \n",
            "  inflating: FinancialPhraseBank/Sentences_50Agree.txt  \n",
            "  inflating: FinancialPhraseBank/Sentences_66Agree.txt  \n",
            "  inflating: FinancialPhraseBank/Sentences_75Agree.txt  \n",
            "  inflating: FinancialPhraseBank/Sentences_AllAgree.txt  \n",
            "  inflating: all-data.csv            \n"
          ]
        }
      ],
      "source": [
        "!unzip Finance.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbDcupw2hvrX",
        "outputId": "4a4c5e36-bad0-4d73-e120-b64442e18379"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-05 09:16:28--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-05-05 09:16:28--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-05-05 09:16:29--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.08MB/s    in 2m 42s  \n",
            "\n",
            "2023-05-05 09:19:11 (5.09 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXppxVNSjD94",
        "outputId": "c05f7136-a8f1-4ba0-90c6-a14afcb54178"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "df=pd.read_csv(\"all-data.csv\", encoding = \"ISO-8859-1\", names =['sentiment' , 'text'])\n",
        "X=df[\"text\"]\n",
        "y=df[\"sentiment\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
      ],
      "metadata": {
        "id": "rZeX8Wyqg2_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee97124a-40c0-4759-f632-9dee0d50bd97"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_model(File):\n",
        "    print(\"Loading Glove Model\")\n",
        "    glove_model = {}\n",
        "    with open(File,'r') as f:\n",
        "        for line in f:\n",
        "            split_line = line.split()\n",
        "            word = split_line[0]\n",
        "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
        "            glove_model[word] = embedding\n",
        "    print(f\"{len(glove_model)} words loaded!\")\n",
        "    return glove_model\n",
        "vectors=load_glove_model('glove.6B.100d.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4wg-lS5uMqI",
        "outputId": "d786f2e5-dd79-4fe1-87af-19a67b5d8300"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Glove Model\n",
            "400000 words loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contractions_dict = {\"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\"}\n",
        "# Regular expression for finding contractions\n",
        "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "# Expanding Contractions in the reviews\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in stopwords_set])\n",
        "\n",
        "stopwords_set = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "def stem_words(text):\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])"
      ],
      "metadata": {
        "id": "lfWBsP9Xxd-o"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=df[\"text\"]\n",
        "y=df[\"sentiment\"]\n",
        "X=X.apply(lambda x: x.lower())\n",
        "X=X.apply(lambda x:expand_contractions(x))\n",
        "X=X.apply(lambda x: re.sub('W*dw*','',x))\n",
        "X=X.apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
        "#X=X.apply(lambda x: remove_stopwords(x))\n",
        "X = X.apply(lambda x: stem_words(x))\n",
        "X = X.apply(lambda text: lemmatize_words(text))\n",
        "X= X.apply(lambda x: re.sub(' +', ' ', x))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
      ],
      "metadata": {
        "id": "cbsZCvggxNAp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_vec(X):\n",
        "  py_nlp_en = English()\n",
        "  py_tokenizer_en = py_nlp_en.tokenizer \n",
        "  value_vector=0\n",
        "  feature_map=[]\n",
        "  for text in X:\n",
        "    words =list(py_tokenizer_en(text))\n",
        "    value_vector=np.zeros(100)\n",
        "    temp=0\n",
        "    for word in words:\n",
        "      word=str(word.text)\n",
        "      if word in vectors:\n",
        "        value_vector=vectors[word]+value_vector\n",
        "        temp=temp+1\n",
        "    feature_map.append(value_vector/temp)\n",
        "  return feature_map\n",
        "feature_maps_train=calculate_vec(X_train)\n",
        "feature_maps_test=calculate_vec(X_test)"
      ],
      "metadata": {
        "id": "qTLSDV2E0vLg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def labler(y):\n",
        "  label_to_int = {\"neutral\": 1, \"positive\": 2, \"negative\": 0}\n",
        "  output_data = list()\n",
        "  for label in y:\n",
        "    output_data.append(label_to_int[label])\n",
        "  output_data = np.array(output_data)\n",
        "  return output_data"
      ],
      "metadata": {
        "id": "SGHDdCTQ8xJK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=labler(y_train)\n",
        "y_test=labler(y_test)"
      ],
      "metadata": {
        "id": "UvRDebnB52xg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "71ZAiXPx6GSy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(solver='sag', multi_class='multinomial')\n",
        "lr.fit(feature_maps_train, y_train)\n",
        "y_pred = lr.predict(feature_maps_train)\n",
        "print(classification_report(y_train, y_pred))\n",
        "\n",
        "y_pred = lr.predict(feature_maps_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3QO2thrjR84",
        "outputId": "5a19ce41-8a25-42bc-d0f7-de897816f431"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.31      0.41       543\n",
            "           1       0.72      0.91      0.81      2606\n",
            "           2       0.60      0.40      0.48      1212\n",
            "\n",
            "    accuracy                           0.69      4361\n",
            "   macro avg       0.64      0.54      0.56      4361\n",
            "weighted avg       0.67      0.69      0.66      4361\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.26      0.40        61\n",
            "           1       0.68      0.91      0.78       273\n",
            "           2       0.69      0.46      0.55       151\n",
            "\n",
            "    accuracy                           0.69       485\n",
            "   macro avg       0.72      0.54      0.58       485\n",
            "weighted avg       0.70      0.69      0.66       485\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf = GaussianNB()\n",
        "clf.fit(feature_maps_train, y_train)\n",
        "y_pred = clf.predict(feature_maps_train)\n",
        "print(classification_report(y_train, y_pred))\n",
        "\n",
        "y_pred = clf.predict(feature_maps_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "up2nGrZUFHRD",
        "outputId": "891361b9-3efd-46d5-96f9-a3fd637383c9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.52      0.37       543\n",
            "           1       0.71      0.75      0.73      2606\n",
            "           2       0.36      0.19      0.25      1212\n",
            "\n",
            "    accuracy                           0.57      4361\n",
            "   macro avg       0.46      0.49      0.45      4361\n",
            "weighted avg       0.56      0.57      0.55      4361\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.48      0.35        61\n",
            "           1       0.67      0.78      0.72       273\n",
            "           2       0.48      0.19      0.27       151\n",
            "\n",
            "    accuracy                           0.56       485\n",
            "   macro avg       0.47      0.48      0.45       485\n",
            "weighted avg       0.56      0.56      0.54       485\n",
            "\n"
          ]
        }
      ]
    }
  ]
}